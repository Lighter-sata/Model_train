#!/usr/bin/env python3
"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±»ç«èµ› - æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è„šæœ¬
åˆå¹¶è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
"""

import os
import re
import json
import torch
import argparse
from typing import Dict, Any, List, Optional
from swift.llm import (
    TrainArguments, sft_main, register_dataset, DatasetMeta, ResponsePreprocessor, SubsetDataset,
    InferArguments, infer_main
)
from swift.utils import read_from_jsonl, write_to_jsonl
from pathlib import Path

os.environ['CUDA_VISIBLE_DEVICES'] = '0'

def get_project_root():
    """è·å–é¡¹ç›®æ ¹ç›®å½•"""
    # ä»å½“å‰è„šæœ¬ä½ç½®å‘ä¸Šä¸¤çº§åˆ°è¾¾é¡¹ç›®æ ¹ç›®å½•
    current_file = Path(__file__).resolve()
    return current_file.parent.parent

class EnhancedPreprocessor(ResponsePreprocessor):
    """ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†å™¨"""

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        query = f"""è¯·åˆ¤æ–­ä¸‹é¢ä¸¤å¥è¯åœ¨é‡‘èè¯­å¢ƒä¸‹æ˜¯å¦è¡¨è¾¾ç›¸åŒçš„è¯­ä¹‰å«ä¹‰ã€‚

å¥å­1: {row['text1']}
å¥å­2: {row['text2']}

è¦æ±‚ï¼š
- å¦‚æœä¸¤å¥è¯å«ä¹‰ç›¸åŒæˆ–éå¸¸ç›¸ä¼¼ï¼Œè¾“å‡º1
- å¦‚æœä¸¤å¥è¯å«ä¹‰ä¸åŒæˆ–ä¸ç›¸ä¼¼ï¼Œè¾“å‡º0
- åªè¾“å‡ºæ•°å­—0æˆ–1ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹

åˆ¤æ–­ç»“æœ: """

        response = str(row['label'])
        row = {
            'query': query,
            'response': response
        }
        return super().preprocess(row)

def register_datasets():
    """æ³¨å†Œæ•°æ®é›†"""
    register_dataset(
        DatasetMeta(
            ms_dataset_id='swift/financial_classification',
            subsets=[
                SubsetDataset('train', split=['train']),
                SubsetDataset('val', split=['train[:1000]']),  # ä½¿ç”¨å‰1000ä¸ªæ ·æœ¬ä½œä¸ºéªŒè¯é›†
                SubsetDataset('test', split=['test'])
            ],
            preprocess_func=EnhancedPreprocessor(),
            dataset_config={
                'trust_remote_code': True,
                'download_mode': 'reuse_dataset_if_exists'
            }
        )
    )

def get_training_args(output_dir: Optional[str] = None) -> TrainArguments:
    """è·å–ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°"""
    if output_dir is None:
        project_root = get_project_root()
        output_dir = str(project_root / 'models' / 'enhanced_output')
    """è·å–ä¼˜åŒ–çš„è®­ç»ƒå‚æ•°"""

    return TrainArguments(
        model='Qwen/Qwen3-4B-Instruct-2507',
        model_type='qwen3',
        dataset=['swift/financial_classification:train'],
        train_type='lora',
        torch_dtype='bfloat16',
        num_train_epochs=5,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=4,
        learning_rate=1.8e-4,
        lr_scheduler_type='cosine',
        warmup_ratio=0.12,
        lora_rank=16,
        lora_alpha=32,
        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
        lora_dropout=0.05,
        gradient_accumulation_steps=8,
        max_grad_norm=1.0,
        max_length=1024,
        save_steps=100,
        eval_steps=100,
        save_total_limit=5,
        logging_steps=20,
        output_dir=output_dir,
        save_only_model=True,
        packing=True,
        dataset_num_proc=4,
        dataloader_num_workers=4,
        attn_impl='flash_attn',
        use_nested_quant=True,
        system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ¤æ–­ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸¤å¥è¯åœ¨é‡‘èè¯­å¢ƒä¸‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œåªè¾“å‡º0æˆ–1ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚",
        seed=42,
        gradient_checkpointing=True,
        ddp_find_unused_parameters=False,
        early_stopping=True,
        early_stopping_patience=3,
        early_stopping_threshold=0.001,
        load_best_model_at_end=True,
    )

def extract_prediction(response: str) -> int:
    """ä»æ¨¡å‹è¾“å‡ºä¸­æå–é¢„æµ‹ç»“æœ"""
    if not response:
        return 0

    response = response.strip()

    # ç›´æ¥åŒ¹é…æ•°å­—0æˆ–1
    if response in ['0', '1']:
        return int(response)

    # åŒ¹é…åŒ…å«æ•°å­—çš„æ¨¡å¼
    digit_match = re.search(r'\b([01])\b', response)
    if digit_match:
        return int(digit_match.group(1))

    # åŸºäºå…³é”®è¯åˆ¤æ–­
    response_lower = response.lower()
    positive_keywords = ['ç›¸ä¼¼', 'ç›¸åŒ', 'ç±»ä¼¼', 'ä¸€è‡´', 'æ˜¯', 'yes', 'true']
    negative_keywords = ['ä¸åŒ', 'ä¸ç›¸ä¼¼', 'ä¸ç›¸åŒ', 'å·®å¼‚', 'ä¸æ˜¯', 'no', 'false']

    positive_score = sum(1 for word in positive_keywords if word in response_lower)
    negative_score = sum(1 for word in negative_keywords if word in response_lower)

    if positive_score > negative_score:
        return 1
    elif negative_score > positive_score:
        return 0
    else:
        return 0

def find_best_checkpoint(output_dir: Optional[str] = None) -> Optional[str]:
    """æŸ¥æ‰¾æœ€ä½³checkpoint"""
    if output_dir is None:
        project_root = get_project_root()
        output_dir = str(project_root / 'models' / 'enhanced_output')
    """æŸ¥æ‰¾æœ€ä½³checkpoint"""
    import glob

    checkpoint_pattern = f"{output_dir}/checkpoint-*"
    checkpoint_dirs = glob.glob(checkpoint_pattern)

    if not checkpoint_dirs:
        print(f"âŒ æœªæ‰¾åˆ°checkpointæ–‡ä»¶: {checkpoint_pattern}")
        return None

    try:
        latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))
        print(f"âœ… æ‰¾åˆ°æœ€ä½³æ¨¡å‹: {latest_checkpoint}")
        return latest_checkpoint
    except ValueError:
        print(f"âš ï¸ æ— æ³•ç¡®å®šæœ€ä½³checkpointï¼Œä½¿ç”¨: {checkpoint_dirs[0]}")
        return checkpoint_dirs[0]

def get_inference_args(ckpt_dir: str) -> InferArguments:
    """è·å–æ¨ç†å‚æ•°"""
    project_root = get_project_root()
    result_path = str(project_root / 'results' / 'enhanced_result.jsonl')

    return InferArguments(
        adapters=[ckpt_dir],
        temperature=0.0,
        max_batch_size=16,
        max_new_tokens=8,
        val_dataset=["swift/financial_classification:test"],
        infer_backend='pt',
        do_sample=False,
        result_path=result_path
    )

def run_training():
    """è¿è¡Œæ¨¡å‹è®­ç»ƒ"""
    print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)

    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        print(f"âœ… GPUå¯ç”¨: {gpu_count} Ã— {gpu_name}")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")
        return False

    # æ³¨å†Œæ•°æ®é›†
    print("\nğŸ“ æ³¨å†Œæ•°æ®é›†...")
    try:
        register_datasets()
        print("âœ… æ•°æ®é›†æ³¨å†ŒæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æ³¨å†Œå¤±è´¥: {e}")
        return False

    # è·å–è®­ç»ƒå‚æ•°
    train_args = get_training_args()
    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  â€¢ æ¨¡å‹: {train_args.model}")
    print(f"  â€¢ è®­ç»ƒè½®æ•°: {train_args.num_train_epochs}")
    print(f"  â€¢ å­¦ä¹ ç‡: {train_args.learning_rate}")
    print(f"  â€¢ LoRA rank: {train_args.lora_rank}")
    print(f"  â€¢ è¾“å‡ºç›®å½•: {train_args.output_dir}")

    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    try:
        sft_main(train_args)
        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        return True
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def run_inference():
    """è¿è¡Œæ¨¡å‹æ¨ç†"""
    print("ğŸ§  å¼€å§‹æ¨¡å‹æ¨ç†")
    print("=" * 50)

    # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
    print("\nğŸ“ æŸ¥æ‰¾æœ€ä½³æ¨¡å‹...")
    ckpt_dir = find_best_checkpoint()
    if not ckpt_dir:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹checkpoint")
        return False

    # æ³¨å†Œæ•°æ®é›†
    print("\nğŸ“ æ³¨å†Œæ¨ç†æ•°æ®é›†...")
    try:
        register_datasets()
        print("âœ… æ•°æ®é›†æ³¨å†ŒæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æ³¨å†Œå¤±è´¥: {e}")
        return False

    # è·å–æ¨ç†å‚æ•°
    infer_args = get_inference_args(ckpt_dir)

    print("ğŸ“‹ æ¨ç†é…ç½®:")
    print(f"  â€¢ æ¨¡å‹: {ckpt_dir}")
    print(f"  â€¢ æ¸©åº¦: {infer_args.temperature}")
    print(f"  â€¢ æ‰¹æ¬¡å¤§å°: {infer_args.max_batch_size}")
    print(f"  â€¢ è¾“å‡ºæ–‡ä»¶: {infer_args.result_path}")

    # å¼€å§‹æ¨ç†
    print("\nğŸ§  å¼€å§‹æ¨ç†...")
    try:
        result = infer_main(infer_args)
        print("âœ… æ¨ç†å®Œæˆï¼")
        return True
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ¨¡å‹è®­ç»ƒå’Œæ¨ç†è„šæœ¬")
    parser.add_argument('action', choices=['train', 'inference', 'all'],
                       help='æ‰§è¡Œæ“ä½œ: train(è®­ç»ƒ), inference(æ¨ç†), all(è®­ç»ƒ+æ¨ç†)')

    args = parser.parse_args()

    success = True

    if args.action in ['train', 'all']:
        success &= run_training()

    if args.action in ['inference', 'all']:
        success &= run_inference()

    if success:
        print("\nğŸ‰ æ“ä½œå®Œæˆï¼")
        if args.action in ['inference', 'all']:
            print("\nğŸ“¤ ç»“æœæ–‡ä»¶:")
            print("  â€¢ æ¨ç†ç»“æœ: results/enhanced_result.jsonl")
            print("  â€¢ ç«èµ›æäº¤: cp results/enhanced_result.jsonl results/result.json")
    else:
        print("\nâŒ æ“ä½œå¤±è´¥")

if __name__ == '__main__':
    main()
