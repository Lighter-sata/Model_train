#!/usr/bin/env python3
"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±»ç«èµ› - æ•°æ®å¤„ç†è„šæœ¬
åˆå¹¶æ•°æ®ä¸‹è½½å’Œåˆ†æåŠŸèƒ½
"""

import os
import json
import requests
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import jieba
from wordcloud import WordCloud
import argparse

def download_dataset_files():
    """ç›´æ¥ä¸‹è½½æ•°æ®é›†æ–‡ä»¶"""

    print("ğŸ” ä¸‹è½½æ•°æ®é›†")
    print("=" * 50)

    # åˆ›å»ºç›®å½•
    os.makedirs('../results/dataset_analysis', exist_ok=True)
    os.makedirs('../data', exist_ok=True)

    try:
        # ä¸‹è½½è®­ç»ƒé›†
        print("ğŸ“¥ ä¸‹è½½è®­ç»ƒé›†...")
        train_url = "https://www.modelscope.cn/api/v1/datasets/swift/financial_classification/repo?Source=SDK&Revision=master&FilePath=train.jsonl"
        response = requests.get(train_url)
        response.raise_for_status()

        train_file = '../data/train.jsonl'
        with open(train_file, 'w', encoding='utf-8') as f:
            f.write(response.text)

        # ä¸‹è½½æµ‹è¯•é›†
        print("ğŸ“¥ ä¸‹è½½æµ‹è¯•é›†...")
        test_url = "https://www.modelscope.cn/api/v1/datasets/swift/financial_classification/repo?Source=SDK&Revision=master&FilePath=test.jsonl"
        response = requests.get(test_url)
        response.raise_for_status()

        test_file = '../data/test.jsonl'
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(response.text)

        print("âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ")
        return train_file, test_file

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None, None

def load_jsonl_data(file_path):
    """åŠ è½½JSONLæ ¼å¼çš„æ•°æ®"""
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line.strip()))
        return data
    except Exception as e:
        print(f"âŒ åŠ è½½{file_path}å¤±è´¥: {e}")
        return []

def analyze_dataset():
    """åˆ†ææ•°æ®é›†"""

    print("\nğŸ“Š åˆ†ææ•°æ®é›†")
    print("=" * 50)

    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    train_file = '../data/train.jsonl'
    test_file = '../data/test.jsonl'

    if not os.path.exists(train_file) or not os.path.exists(test_file):
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®ä¸‹è½½")
        return

    # åŠ è½½æ•°æ®
    print("ğŸ“¥ åŠ è½½æ•°æ®...")
    train_data = load_jsonl_data(train_file)
    test_data = load_jsonl_data(test_file)

    print(f"è®­ç»ƒé›†: {len(train_data)} æ¡")
    print(f"æµ‹è¯•é›†: {len(test_data)} æ¡")

    if not train_data:
        return

    # åŸºæœ¬ä¿¡æ¯
    print("\nğŸ“‹ æ•°æ®é›†åŸºæœ¬ä¿¡æ¯")
    print("-" * 30)
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)}")
    print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
    print(f"ç‰¹å¾å­—æ®µ: {list(train_data[0].keys())}")

    # ç¤ºä¾‹
    print("\nğŸ” ç¤ºä¾‹æ ·æœ¬")
    for i in range(min(3, len(train_data))):
        sample = train_data[i]
        print(f"æ ·æœ¬ {i+1}:")
        print(f"  text1: {sample['text1']}")
        print(f"  text2: {sample['text2']}")
        print(f"  label: {sample['label']}")
        print()

    # ç±»åˆ«åˆ†å¸ƒ
    print("ğŸ“ˆ ç±»åˆ«åˆ†å¸ƒåˆ†æ")
    print("-" * 30)

    train_labels = [sample['label'] for sample in train_data]
    train_counter = Counter(train_labels)
    print(f"è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ: {dict(train_counter)}")

    test_labels = [sample.get('label') for sample in test_data if sample.get('label') is not None]
    test_counter = Counter(test_labels)
    print(f"æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ: {dict(test_counter)}")

    # å¯è§†åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    ax1.bar(train_counter.keys(), train_counter.values(), color=['skyblue', 'lightcoral'])
    ax1.set_title('è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ')
    ax1.set_xlabel('ç±»åˆ«')
    ax1.set_ylabel('æ ·æœ¬æ•°')

    ax2.bar(test_counter.keys(), test_counter.values(), color=['skyblue', 'lightcoral'])
    ax2.set_title('æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ')
    ax2.set_xlabel('ç±»åˆ«')
    ax2.set_ylabel('æ ·æœ¬æ•°')

    plt.tight_layout()
    plt.savefig('../results/dataset_analysis/class_distribution.png', dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ… ç±»åˆ«åˆ†å¸ƒå›¾å·²ä¿å­˜")

    # æ–‡æœ¬é•¿åº¦åˆ†æ
    print("\nğŸ“ æ–‡æœ¬é•¿åº¦åˆ†æ")
    print("-" * 30)

    train_text1_lens = [len(sample['text1']) for sample in train_data]
    train_text2_lens = [len(sample['text2']) for sample in train_data]

    print("è®­ç»ƒé›†text1é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡: {sum(train_text1_lens)/len(train_text1_lens):.1f}")
    print(f"  æœ€å¤§: {max(train_text1_lens)}")
    print(f"  æœ€å°: {min(train_text1_lens)}")

    print("è®­ç»ƒé›†text2é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡: {sum(train_text2_lens)/len(train_text2_lens):.1f}")
    print(f"  æœ€å¤§: {max(train_text2_lens)}")
    print(f"  æœ€å°: {min(train_text2_lens)}")

    # è¯æ±‡åˆ†æ
    print("\nğŸ“ è¯æ±‡åˆ†æ")
    print("-" * 30)

    all_words = []
    word_freq = Counter()

    for sample in tqdm(train_data[:1000], desc="åˆ†è¯å¤„ç†"):  # åªå¤„ç†å‰1000ä¸ªæ ·æœ¬
        words1 = jieba.cut(sample['text1'])
        words2 = jieba.cut(sample['text2'])

        for word in words1:
            if len(word.strip()) > 1:
                word_freq[word] += 1
                all_words.append(word)

        for word in words2:
            if len(word.strip()) > 1:
                word_freq[word] += 1
                all_words.append(word)

    top_words = word_freq.most_common(20)
    print("é«˜é¢‘è¯æ±‡TOP 20:")
    for word, freq in top_words:
        print(f"{word}: {freq}")

    # è¯äº‘
    try:
        wordcloud = WordCloud(
            width=800, height=400,
            background_color='white',
            max_words=100
        ).generate_from_frequencies(dict(top_words))

        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('é«˜é¢‘è¯æ±‡è¯äº‘å›¾')
        plt.savefig('../results/dataset_analysis/wordcloud.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… è¯äº‘å›¾å·²ä¿å­˜")
    except Exception as e:
        print(f"è¯äº‘å›¾ç”Ÿæˆå¤±è´¥: {e}")

    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report = {
        'dataset_info': {
            'train_size': len(train_data),
            'test_size': len(test_data),
            'features': list(train_data[0].keys())
        },
        'class_distribution': {
            'train': dict(train_counter),
            'test': dict(test_counter)
        },
        'text_stats': {
            'train_text1_avg_len': sum(train_text1_lens)/len(train_text1_lens),
            'train_text2_avg_len': sum(train_text2_lens)/len(train_text2_lens)
        },
        'vocabulary': {
            'total_words': len(all_words),
            'unique_words': len(word_freq),
            'top_words': top_words[:10]
        }
    }

    with open('../results/dataset_analysis/analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print("âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜")

    print("\nğŸ‰ æ•°æ®é›†å¤„ç†å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®å¤„ç†è„šæœ¬")
    parser.add_argument('action', choices=['download', 'analyze', 'all'],
                       help='æ‰§è¡Œæ“ä½œ: download(ä¸‹è½½), analyze(åˆ†æ), all(å…¨éƒ¨)')

    args = parser.parse_args()

    if args.action in ['download', 'all']:
        train_file, test_file = download_dataset_files()
        if not train_file:
            return

    if args.action in ['analyze', 'all']:
        analyze_dataset()

if __name__ == '__main__':
    main()
