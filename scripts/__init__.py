"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±»ç«èµ› - è„šæœ¬åŒ…
"""

# åœ¨å¯¼å…¥ä»»ä½•å¯èƒ½ä¾èµ–datasetsçš„åº“ä¹‹å‰ï¼Œå…ˆä¿®å¤datasetså…¼å®¹æ€§é—®é¢˜
def fix_datasets_import():
    """ä¿®å¤datasetså¯¼å…¥é—®é¢˜"""
    try:
        import datasets
        if not hasattr(datasets, 'LargeList'):
            # å°è¯•ä»featureså¯¼å…¥
            try:
                from datasets.features import Sequence
                datasets.LargeList = Sequence
                print("ğŸ”§ [scripts] å·²è‡ªåŠ¨ä¿®å¤datasets LargeListå¯¼å…¥é—®é¢˜")
            except ImportError:
                # åˆ›å»ºåŸºç¡€å…¼å®¹ç±»
                class LargeList:
                    pass
                datasets.LargeList = LargeList
                print("ğŸ”§ [scripts] å·²åˆ›å»ºdatasets LargeListå…¼å®¹ç±»")

        # ä¿®å¤_FEATURE_TYPES
        from datasets.features import features
        if not hasattr(features, '_FEATURE_TYPES'):
            print("ğŸ”§ [scripts] _FEATURE_TYPESä¸å­˜åœ¨ï¼Œå¼€å§‹ä¿®å¤...")

            # åˆ›å»ºæ‰€æœ‰featureç±»å‹çš„å­—å…¸
            _FEATURE_TYPES = {}
            for attr_name in dir(features):
                attr = getattr(features, attr_name)
                if (hasattr(attr, '__name__') and
                    hasattr(attr, '__module__') and
                    attr.__module__ == 'datasets.features.features' and
                    (attr_name.endswith('Type') or 'Array' in attr_name or 'Value' in attr_name or 'Class' in attr_name)):
                    _FEATURE_TYPES[attr_name] = attr

            # æ‰‹åŠ¨æ·»åŠ ä¸€äº›é‡è¦çš„ç±»å‹
            if hasattr(features, 'Sequence'):
                _FEATURE_TYPES['LargeList'] = features.Sequence

            # å°†å…¶æ·»åŠ åˆ°featuresæ¨¡å—
            features._FEATURE_TYPES = _FEATURE_TYPES
            print(f"ğŸ”§ [scripts] å·²åˆ›å»º_FEATURE_TYPES ({len(_FEATURE_TYPES)}ä¸ªç±»å‹)")

        # ä¿®å¤exceptionsæ¨¡å—
        if not hasattr(datasets, 'exceptions'):
            print("ğŸ”§ [scripts] exceptionsæ¨¡å—ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¿®å¤...")
            import types
            exceptions_module = types.ModuleType('datasets.exceptions')

            exception_classes = [
                'DatasetNotFoundError', 'DatasetBuildError', 'DatasetGenerationError',
                'DatasetValidationError', 'NonMatchingChecksumError', 'DatasetInfoError',
                'DataFilesNotFoundError', 'EmptyDatasetError', 'ManualDownloadError',
                'DatasetNotImplementedError', 'DatasetOnlineError', 'DatasetOfflineError',
                'StreamingError', 'CorruptedFileError', 'SplitNotFoundError'
            ]

            for exc_name in exception_classes:
                exc_class = type(exc_name, (Exception,), {})
                setattr(exceptions_module, exc_name, exc_class)

            datasets.exceptions = exceptions_module
            import sys
            sys.modules['datasets.exceptions'] = exceptions_module
            print("ğŸ”§ [scripts] å·²åˆ›å»ºexceptionsæ¨¡å—")

        # ä¿®å¤HubDatasetModuleFactoryWithParquetExport
        from datasets import load
        if not hasattr(load, 'HubDatasetModuleFactoryWithParquetExport'):
            print("ğŸ”§ [scripts] HubDatasetModuleFactoryWithParquetExportä¸å­˜åœ¨ï¼Œå¼€å§‹ä¿®å¤...")
            from datasets.load import HubDatasetModuleFactoryWithoutScript

            class HubDatasetModuleFactoryWithParquetExport(HubDatasetModuleFactoryWithoutScript):
                def __init__(self, *args, **kwargs):
                    super().__init__(*args, **kwargs)
                    self.supports_parquet_export = True

            load.HubDatasetModuleFactoryWithParquetExport = HubDatasetModuleFactoryWithParquetExport
            print("ğŸ”§ [scripts] å·²åˆ›å»ºHubDatasetModuleFactoryWithParquetExportå…¼å®¹ç±»")

        # ä¿®å¤_get_importable_file_path
        if not hasattr(load, '_get_importable_file_path'):
            print("ğŸ”§ [scripts] _get_importable_file_pathä¸å­˜åœ¨ï¼Œå¼€å§‹ä¿®å¤...")

            def _get_importable_file_path(dataset_name, filename, use_auth_token=None):
                return f'{dataset_name}/{filename}'

            load._get_importable_file_path = _get_importable_file_path
            print("ğŸ”§ [scripts] å·²åˆ›å»º_get_importable_file_pathå…¼å®¹å‡½æ•°")

        # ä¿®å¤resolve_trust_remote_code
        if not hasattr(load, 'resolve_trust_remote_code'):
            print("ğŸ”§ [scripts] resolve_trust_remote_codeä¸å­˜åœ¨ï¼Œå¼€å§‹ä¿®å¤...")

            def resolve_trust_remote_code(trust_remote_code, repo_id=None):
                return trust_remote_code

            load.resolve_trust_remote_code = resolve_trust_remote_code
            print("ğŸ”§ [scripts] å·²åˆ›å»ºresolve_trust_remote_codeå…¼å®¹å‡½æ•°")

    except ImportError:
        pass

# è¿è¡Œä¿®å¤
fix_datasets_import()

from .data_processor import download_dataset_files, analyze_dataset
from .model_trainer import run_training, run_inference
from .evaluate import main as evaluate_main
from .utils import clean_prediction_output, calculate_text_similarity

__version__ = "1.0.0"
__all__ = [
    'download_dataset_files',
    'analyze_dataset',
    'run_training',
    'run_inference',
    'evaluate_main',
    'clean_prediction_output',
    'calculate_text_similarity'
]
