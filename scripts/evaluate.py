#!/usr/bin/env python3
"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±»ç«èµ› - è¯„ä¼°è„šæœ¬
è®¡ç®—å‡†ç¡®ç‡ã€æ··æ·†çŸ©é˜µã€åˆ†ç±»æŠ¥å‘Šï¼Œå¹¶åˆ†æé”™è¯¯æ ·æœ¬
"""

import os
import json
import numpy as np
from typing import Dict, List, Tuple
from collections import Counter, defaultdict
from swift.utils import read_from_jsonl
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)

def load_test_labels() -> List[int]:
    """åŠ è½½æµ‹è¯•é›†æ ‡ç­¾"""

    try:
        from datasets import load_dataset
        labels = load_dataset('json', data_files='test_label.jsonl', split='train')['label']
        return labels
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½æµ‹è¯•æ ‡ç­¾: {e}")
        print("è¯·ç¡®ä¿test_label.jsonlæ–‡ä»¶å­˜åœ¨")
        return []

def load_predictions(result_file: str) -> Tuple[List[int], List[Dict]]:
    """åŠ è½½é¢„æµ‹ç»“æœ"""

    if not os.path.exists(result_file):
        print(f"âŒ æ‰¾ä¸åˆ°ç»“æœæ–‡ä»¶: {result_file}")
        return [], []

    try:
        predictions_data = read_from_jsonl(result_file)
        predictions = []

        for pred in predictions_data:
            # æå–é¢„æµ‹å€¼
            response = pred.get('response', '').strip()
            if response in ['0', '1']:
                predictions.append(int(response))
            else:
                # å°è¯•ä»å…¶ä»–å­—æ®µæå–
                prediction = pred.get('prediction')
                if prediction is not None:
                    predictions.append(int(prediction))
                else:
                    print(f"âš ï¸ æ— æ³•è§£æé¢„æµ‹ç»“æœ: {pred}")
                    predictions.append(0)  # é»˜è®¤å€¼

        return predictions, predictions_data

    except Exception as e:
        print(f"âŒ åŠ è½½é¢„æµ‹ç»“æœå¤±è´¥: {e}")
        return [], []

def calculate_metrics(y_true: List[int], y_pred: List[int]) -> Dict:
    """è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡"""

    metrics = {}

    try:
        # åŸºæœ¬æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision_macro'] = precision_score(y_true, y_pred, average='macro')
        metrics['recall_macro'] = recall_score(y_true, y_pred, average='macro')
        metrics['f1_macro'] = f1_score(y_true, y_pred, average='macro')
        metrics['precision_micro'] = precision_score(y_true, y_pred, average='micro')
        metrics['recall_micro'] = recall_score(y_true, y_pred, average='micro')
        metrics['f1_micro'] = f1_score(y_true, y_pred, average='micro')

        # ç±»åˆ«ç‰¹å®šçš„æŒ‡æ ‡
        metrics['precision_class_0'] = precision_score(y_true, y_pred, pos_label=0)
        metrics['precision_class_1'] = precision_score(y_true, y_pred, pos_label=1)
        metrics['recall_class_0'] = recall_score(y_true, y_pred, pos_label=0)
        metrics['recall_class_1'] = recall_score(y_true, y_pred, pos_label=1)
        metrics['f1_class_0'] = f1_score(y_true, y_pred, pos_label=0)
        metrics['f1_class_1'] = f1_score(y_true, y_pred, pos_label=1)

        # è®¡ç®—AUCï¼ˆå¦‚æœéœ€è¦æ¦‚ç‡å€¼ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        # å¯¹äºäºŒåˆ†ç±»ï¼ŒAUCå¯ä»¥ä½¿ç”¨é¢„æµ‹å€¼è¿‘ä¼¼è®¡ç®—
        try:
            metrics['auc'] = roc_auc_score(y_true, y_pred)
        except:
            metrics['auc'] = None

    except Exception as e:
        print(f"âš ï¸ è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")

    return metrics

def plot_confusion_matrix(y_true: List[int], y_pred: List[int], save_path: str = 'evaluation_results/confusion_matrix.png'):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))

    # ä½¿ç”¨seabornç»˜åˆ¶çƒ­åŠ›å›¾
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['ä¸ç›¸ä¼¼(0)', 'ç›¸ä¼¼(1)'],
                yticklabels=['ä¸ç›¸ä¼¼(0)', 'ç›¸ä¼¼(1)'])

    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')

    # æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
    accuracy = np.trace(cm) / np.sum(cm)
    plt.text(0.5, -0.1, '.3f',
             ha='center', va='center', transform=plt.gca().transAxes,
             fontsize=12, fontweight='bold')

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print("ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³:")
    print(f"  {save_path}")

def analyze_errors(y_true: List[int], y_pred: List[int], predictions_data: List[Dict]) -> Dict:
    """åˆ†æé”™è¯¯æ ·æœ¬"""

    error_analysis = {
        'total_errors': 0,
        'error_types': {'FP': 0, 'FN': 0, 'other': 0},  # FP: å‡æ­£ä¾‹, FN: å‡è´Ÿä¾‹
        'error_samples': []
    }

    # æ‰¾å‡ºé”™è¯¯æ ·æœ¬
    for i, (true, pred) in enumerate(zip(y_true, y_pred)):
        if true != pred:
            error_analysis['total_errors'] += 1

            # åˆ†ç±»é”™è¯¯ç±»å‹
            if true == 0 and pred == 1:  # å‡æ­£ä¾‹ï¼šé¢„æµ‹ç›¸ä¼¼ä½†å®é™…ä¸ç›¸ä¼¼
                error_type = 'FP'
            elif true == 1 and pred == 0:  # å‡è´Ÿä¾‹ï¼šé¢„æµ‹ä¸ç›¸ä¼¼ä½†å®é™…ç›¸ä¼¼
                error_type = 'FN'
            else:
                error_type = 'other'

            error_analysis['error_types'][error_type] += 1

            # ä¿å­˜é”™è¯¯æ ·æœ¬è¯¦æƒ…
            if len(error_analysis['error_samples']) < 50:  # åªä¿å­˜å‰50ä¸ªé”™è¯¯æ ·æœ¬
                sample_info = {
                    'index': i,
                    'true_label': true,
                    'pred_label': pred,
                    'error_type': error_type
                }

                # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if i < len(predictions_data):
                    pred_data = predictions_data[i]
                    query = pred_data.get('query', '')

                    # ä»queryä¸­æå–å¥å­
                    import re
                    text1_match = re.search(r'å¥å­1:\s*(.*?)(?:\n|$)', query, re.DOTALL)
                    text2_match = re.search(r'å¥å­2:\s*(.*?)(?:\n|$)', query, re.DOTALL)

                    if text1_match and text2_match:
                        sample_info['text1'] = text1_match.group(1).strip()
                        sample_info['text2'] = text2_match.group(1).strip()

                error_analysis['error_samples'].append(sample_info)

    return error_analysis

def plot_error_analysis(error_analysis: Dict, save_path: str = 'evaluation_results/error_analysis.png'):
    """ç»˜åˆ¶é”™è¯¯åˆ†æå›¾è¡¨"""

    if error_analysis['total_errors'] == 0:
        print("ğŸ‰ æ²¡æœ‰é”™è¯¯æ ·æœ¬ï¼Œæ— éœ€ç»˜åˆ¶é”™è¯¯åˆ†æå›¾")
        return

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # é”™è¯¯ç±»å‹åˆ†å¸ƒ
    error_types = error_analysis['error_types']
    ax1.bar(error_types.keys(), error_types.values(), color=['skyblue', 'lightcoral', 'lightgreen'])
    ax1.set_title('é”™è¯¯ç±»å‹åˆ†å¸ƒ')
    ax1.set_xlabel('é”™è¯¯ç±»å‹')
    ax1.set_ylabel('æ ·æœ¬æ•°')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (k, v) in enumerate(error_types.items()):
        ax1.text(i, v + 0.5, str(v), ha='center', va='bottom')

    # é”™è¯¯ç‡éšæ—¶é—´çš„å˜åŒ–ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿæ ·æœ¬ï¼‰
    errors_by_position = []
    window_size = 100

    # ç®€å•çš„æ—¶é—´åºåˆ—é”™è¯¯åˆ†æ
    error_positions = [i for i, (true, pred) in enumerate(zip(error_analysis.get('y_true', []),
                                                               error_analysis.get('y_pred', [])))
                      if true != pred]

    if len(error_positions) > 10:
        # è®¡ç®—æ»‘åŠ¨çª—å£é”™è¯¯ç‡
        total_samples = len(error_analysis.get('y_true', []))
        error_rates = []

        for start in range(0, total_samples, window_size):
            end = min(start + window_size, total_samples)
            window_errors = sum(1 for pos in error_positions if start <= pos < end)
            window_rate = window_errors / (end - start)
            error_rates.append(window_rate)

        ax2.plot(range(len(error_rates)), error_rates, marker='o')
        ax2.set_title(f'æ»‘åŠ¨çª—å£é”™è¯¯ç‡ (çª—å£å¤§å°={window_size})')
        ax2.set_xlabel('çª—å£ç¼–å·')
        ax2.set_ylabel('é”™è¯¯ç‡')
        ax2.grid(True, alpha=0.3)
    else:
        ax2.text(0.5, 0.5, 'é”™è¯¯æ ·æœ¬è¿‡å°‘\næ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ',
                ha='center', va='center', transform=ax2.transAxes, fontsize=12)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

    print("ğŸ“Š é”™è¯¯åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³:")
    print(f"  {save_path}")

def save_evaluation_report(metrics: Dict, error_analysis: Dict,
                          result_file: str, output_dir: str = 'evaluation_results'):
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""

    os.makedirs(output_dir, exist_ok=True)

    report = {
        'evaluation_summary': {
            'result_file': result_file,
            'total_samples': len(error_analysis.get('y_true', [])),
            'accuracy': metrics.get('accuracy', 0),
            'evaluation_time': str(error_analysis.get('timestamp', 'unknown'))
        },
        'metrics': metrics,
        'error_analysis': {
            'total_errors': error_analysis['total_errors'],
            'error_rate': error_analysis['total_errors'] / len(error_analysis.get('y_true', [])) if error_analysis.get('y_true') else 0,
            'error_types': error_analysis['error_types']
        }
    }

    # ä¿å­˜JSONæŠ¥å‘Š
    report_path = f"{output_dir}/evaluation_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # ä¿å­˜è¯¦ç»†é”™è¯¯æ ·æœ¬
    if error_analysis['error_samples']:
        error_samples_path = f"{output_dir}/error_samples.json"
        with open(error_samples_path, 'w', encoding='utf-8') as f:
            json.dump(error_analysis['error_samples'], f, ensure_ascii=False, indent=2)

    print("ğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜:")
    print(f"  â€¢ å®Œæ•´æŠ¥å‘Š: {report_path}")
    if error_analysis['error_samples']:
        print(f"  â€¢ é”™è¯¯æ ·æœ¬: {error_samples_path}")

def print_evaluation_summary(metrics: Dict, error_analysis: Dict):
    """æ‰“å°è¯„ä¼°æ‘˜è¦"""

    print("\n" + "="*60)
    print("ğŸ¯ æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*60)

    print(f"ğŸ¯ å‡†ç¡®ç‡: {metrics.get('accuracy', 0):.4f}")

    print("\nğŸ“Š è¯¦ç»†æŒ‡æ ‡:")
    print(f"  ç²¾ç¡®ç‡: {metrics.get('precision_macro', 0):.4f}")
    print(f"  å¬å›ç‡: {metrics.get('recall_macro', 0):.4f}")
    print(f"  F1å€¼: {metrics.get('f1_macro', 0):.4f}")
    print(f"  ç²¾ç¡®ç‡(å¾®å¹³å‡): {metrics.get('precision_micro', 0):.4f}")
    print(f"  å¬å›ç‡(å¾®å¹³å‡): {metrics.get('recall_micro', 0):.4f}")
    print(f"  F1å€¼(å¾®å¹³å‡): {metrics.get('f1_micro', 0):.4f}")

    if metrics.get('auc'):
        print(f"  AUC: {metrics.get('auc', 0):.4f}")
    print("\nğŸ·ï¸ ç±»åˆ«ç‰¹å®šæŒ‡æ ‡:")
    print(f"  â€¢ ç±»åˆ«0 (ä¸ç›¸ä¼¼) - ç²¾ç¡®ç‡: {metrics.get('precision_class_0', 0):.4f}, å¬å›ç‡: {metrics.get('recall_class_0', 0):.4f}, F1: {metrics.get('f1_class_0', 0):.4f}")
    print(f"  â€¢ ç±»åˆ«1 (ç›¸ä¼¼) - ç²¾ç¡®ç‡: {metrics.get('precision_class_1', 0):.4f}, å¬å›ç‡: {metrics.get('recall_class_1', 0):.4f}, F1: {metrics.get('f1_class_1', 0):.4f}")

    print(f"\nâŒ é”™è¯¯åˆ†æ:")
    print(f"  â€¢ æ€»é”™è¯¯æ•°: {error_analysis['total_errors']}")
    print(".2f")
    print(f"  â€¢ FP (å‡æ­£ä¾‹): {error_analysis['error_types']['FP']} - é¢„æµ‹ç›¸ä¼¼ä½†å®é™…ä¸ç›¸ä¼¼")
    print(f"  â€¢ FN (å‡è´Ÿä¾‹): {error_analysis['error_types']['FN']} - é¢„æµ‹ä¸ç›¸ä¼¼ä½†å®é™…ç›¸ä¼¼")

    # åŸºçº¿å¯¹æ¯”
    baseline_acc = 0.764
    current_acc = metrics.get('accuracy', 0)
    improvement = current_acc - baseline_acc

    print(f"\nğŸ† ä¸åŸºçº¿å¯¹æ¯”:")
    print(".4f")
    print(".4f")
    if improvement > 0:
        print(".4f")
    elif improvement < 0:
        print(".4f")
    else:
        print("  â€¢ ä¸åŸºçº¿æŒå¹³")

def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""

    print("ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°")
    print("=" * 60)

    # æ£€æŸ¥ç»“æœæ–‡ä»¶
    result_files = ['enhanced_result.jsonl', 'enhanced_result.json', 'result.jsonl', 'result.json']

    result_file = None
    for file in result_files:
        if os.path.exists(file):
            result_file = file
            break

    if not result_file:
        print("âŒ æœªæ‰¾åˆ°é¢„æµ‹ç»“æœæ–‡ä»¶")
        print("è¯·å…ˆè¿è¡Œæ¨ç†è„šæœ¬ç”Ÿæˆç»“æœæ–‡ä»¶")
        print("å°è¯•çš„æ–‡ä»¶:", result_files)
        return

    print(f"âœ… æ‰¾åˆ°ç»“æœæ–‡ä»¶: {result_file}")

    # åŠ è½½æµ‹è¯•æ ‡ç­¾
    print("\nğŸ“¥ åŠ è½½æµ‹è¯•æ ‡ç­¾...")
    y_true = load_test_labels()
    if not y_true:
        return

    # åŠ è½½é¢„æµ‹ç»“æœ
    print("\nğŸ“¥ åŠ è½½é¢„æµ‹ç»“æœ...")
    y_pred, predictions_data = load_predictions(result_file)
    if not y_pred:
        return

    # æ£€æŸ¥é•¿åº¦ä¸€è‡´æ€§
    if len(y_true) != len(y_pred):
        print(f"âŒ æ ‡ç­¾å’Œé¢„æµ‹é•¿åº¦ä¸åŒ¹é…: {len(y_true)} vs {len(y_pred)}")
        return

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {len(y_true)} ä¸ªæµ‹è¯•æ ·æœ¬")

    # è®¡ç®—æŒ‡æ ‡
    print("\nğŸ§® è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    metrics = calculate_metrics(y_true, y_pred)

    # é”™è¯¯åˆ†æ
    print("\nğŸ” åˆ†æé”™è¯¯æ ·æœ¬...")
    error_analysis = analyze_errors(y_true, y_pred, predictions_data)
    error_analysis['y_true'] = y_true  # ä¿å­˜ç”¨äºç»˜å›¾
    error_analysis['y_pred'] = y_pred

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'evaluation_results'
    os.makedirs(output_dir, exist_ok=True)

    # ç”Ÿæˆå¯è§†åŒ–
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_confusion_matrix(y_true, y_pred, f"{output_dir}/confusion_matrix.png")
    plot_error_analysis(error_analysis, f"{output_dir}/error_analysis.png")

    # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    save_evaluation_report(metrics, error_analysis, result_file, output_dir)

    # æ‰“å°æ€»ç»“
    print_evaluation_summary(metrics, error_analysis)

    print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}/")

    # æä¾›å»ºè®®
    accuracy = metrics.get('accuracy', 0)
    if accuracy >= 0.90:
        print("ğŸ† ä¼˜ç§€ï¼å‡†ç¡®ç‡è¾¾åˆ°90%ä»¥ä¸Šï¼Œæ’åæœ‰æœ›è¿›å…¥å‰20åï¼")
    elif accuracy >= 0.85:
        print("ğŸ¯ è‰¯å¥½ï¼å‡†ç¡®ç‡è¾¾åˆ°85%ä»¥ä¸Šï¼Œæœ‰æœ›è¿›å…¥å‰30åï¼")
    elif accuracy >= 0.80:
        print("ğŸ‘ ä¸é”™ï¼å‡†ç¡®ç‡è¾¾åˆ°80%ä»¥ä¸Šï¼Œç»§ç»­ä¼˜åŒ–ï¼")
    else:
        print("ğŸ“š éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œå¯ä»¥å°è¯•è°ƒæ•´Promptæˆ–è¶…å‚æ•°")

if __name__ == '__main__':
    main()
