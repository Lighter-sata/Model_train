#!/usr/bin/env python3
"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±» - åŸºç¡€PyTorchç‰ˆæœ¬
ä¸ä¾èµ–Swiftåº“ï¼Œç›´æ¥ä½¿ç”¨transformersè®­ç»ƒ
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments, DataCollatorWithPadding
)
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# è®¾ç½®GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class FinancialSimilarityDataset(Dataset):
    """é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦æ•°æ®é›†"""

    def __init__(self, tokenizer, data, max_length=512):
        self.tokenizer = tokenizer
        self.data = data
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # æ„å»ºè¾“å…¥æ–‡æœ¬
        text = f"åˆ¤æ–­ä»¥ä¸‹ä¸¤å¥è¯æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼ï¼šå¥å­1: {item['text1']} å¥å­2: {item['text2']}"

        # ç¼–ç 
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(item['label'], dtype=torch.long)
        }

def compute_metrics(eval_pred):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')

    return {
        'accuracy': accuracy,
        'f1': f1
    }

def main():
    print("ğŸš€ é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±» - åŸºç¡€PyTorchç‰ˆæœ¬")
    print("ğŸ¯ ç›®æ ‡å‡†ç¡®ç‡: 0.85+")
    print("ğŸ¤– ä½¿ç”¨æ¨¡å‹: Qwen2-7B (åˆ†ç±»å¤´)")
    print("="*50)

    # 1. åŠ è½½æ•°æ®
    print("ğŸ“š åŠ è½½æ•°æ®é›†...")
    try:
        dataset = load_dataset('swift/financial_classification')
        train_data = dataset['train']
        test_data = dataset['test']

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        train_list = []
        for item in train_data:
            train_list.append({
                'text1': item['text1'],
                'text2': item['text2'],
                'label': item['label']
            })

        test_list = []
        for item in test_data:
            test_list.append({
                'text1': item['text1'],
                'text2': item['text2'],
                'label': item['label']
            })

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train_list)}, æµ‹è¯•é›†: {len(test_list)}")

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿æ•°æ®é›†å¯ç”¨æˆ–æ‰‹åŠ¨ä¸‹è½½")
        return

    # 2. åŠ è½½æ¨¡å‹å’Œtokenizer
    print("ğŸ¤– åŠ è½½Qwen2-7Bæ¨¡å‹...")
    try:
        model_name = "Qwen/Qwen2-7B-Instruct"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.pad_token = tokenizer.eos_token

        model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=2,
            torch_dtype=torch.bfloat16,
        )

        # è°ƒæ•´æ¨¡å‹ä»¥é€‚åº”åˆ†ç±»ä»»åŠ¡
        if hasattr(model, 'score'):
            # Qwenæ¨¡å‹çš„åˆ†ç±»å¤´è°ƒæ•´
            model.score = nn.Linear(model.config.hidden_size, 2)

        model.to(device)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. åˆ›å»ºæ•°æ®é›†
    print("ğŸ”§ åˆ›å»ºæ•°æ®é›†...")
    train_dataset = FinancialSimilarityDataset(tokenizer, train_list)
    test_dataset = FinancialSimilarityDataset(tokenizer, test_list)

    # 4. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir='./results_qwen2_7b_basic',
        num_train_epochs=5,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°=8
        learning_rate=5e-5,
        warmup_ratio=0.1,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_steps=500,
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        fp16=False,  # ä½¿ç”¨bf16
        bf16=True,
        dataloader_num_workers=2,
        remove_unused_columns=False,
    )

    # 5. åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics,
        data_collator=DataCollatorWithPadding(tokenizer),
    )

    # 6. å¼€å§‹è®­ç»ƒ
    print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    print("ğŸ’¡ è¿™å°†éœ€è¦çº¦2-3å°æ—¶ï¼Œå…·ä½“å–å†³äºç¡¬ä»¶")
    print("="*50)

    try:
        trainer.train()
        print("âœ… è®­ç»ƒå®Œæˆï¼")

        # 7. è¯„ä¼°
        print("ğŸ“Š æœ€ç»ˆè¯„ä¼°...")
        eval_results = trainer.evaluate()
        print(f"ğŸ¯ å‡†ç¡®ç‡: {eval_results['eval_accuracy']:.4f}")
        print(f"ğŸ¯ F1å¾—åˆ†: {eval_results['eval_f1']:.4f}")

        # 8. ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model('./best_model_qwen2_7b')
        tokenizer.save_pretrained('./best_model_qwen2_7b')
        print("âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: ./best_model_qwen2_7b")

    except KeyboardInterrupt:
        print("â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
