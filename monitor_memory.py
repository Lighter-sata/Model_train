#!/usr/bin/env python3
"""
æ˜¾å­˜ç›‘æ§å’Œä¼˜åŒ–å»ºè®®è„šæœ¬
"""

import subprocess
import sys
import os

def run_command(cmd, desc=""):
    """è¿è¡Œå‘½ä»¤"""
    try:
        result = subprocess.run(cmd, shell=True, check=True,
                              capture_output=True, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        return None

def get_gpu_info():
    """è·å–GPUä¿¡æ¯"""
    print("ğŸ® GPUä¿¡æ¯:")

    # æ£€æŸ¥nvidia-smi
    nvidia_output = run_command("nvidia-smi --query-gpu=name,memory.total,memory.free,memory.used --format=csv,noheader,nounits")

    if nvidia_output:
        lines = nvidia_output.strip().split('\n')
        for i, line in enumerate(lines):
            name, total, free, used = line.split(', ')
            print(f"  GPU {i}: {name}")
            print(f"    æ€»æ˜¾å­˜: {total} MB")
            print(f"    å·²ä½¿ç”¨: {used} MB")
            print(f"    å¯ç”¨: {free} MB")
            print(f"    ä½¿ç”¨ç‡: {int(used)/int(total)*100:.1f}%")
    else:
        print("  âŒ æ— æ³•è·å–GPUä¿¡æ¯ (nvidia-smiä¸å¯ç”¨)")

def estimate_memory_usage():
    """ä¼°ç®—æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    print("\nğŸ’¾ æ˜¾å­˜ä½¿ç”¨ä¼°ç®—:")

    # 7Bæ¨¡å‹çš„åŸºæœ¬ä¿¡æ¯
    model_params = 7_000_000_000  # 70äº¿å‚æ•°
    param_size = 2  # bfloat16 = 2å­—èŠ‚

    # LoRAå‚æ•°
    lora_rank = 64
    lora_params = model_params * lora_rank * 2 / 1_000_000  # ç™¾ä¸‡å‚æ•°

    print(f"  â€¢ æ¨¡å‹å‚æ•°: {model_params:,} ({model_params * param_size / 1024**3:.1f} GB)")
    print(f"  â€¢ LoRAå‚æ•°: ~{lora_params:.0f}M")
    print(f"  â€¢ æ¢¯åº¦å ç”¨: {model_params * param_size / 1024**3:.1f} GB")
    print(f"  â€¢ ä¼˜åŒ–å™¨çŠ¶æ€: {model_params * param_size * 2 / 1024**3:.1f} GB")

    # ä¼°ç®—æ€»å ç”¨
    base_memory = model_params * param_size / 1024**3  # æ¨¡å‹
    grad_memory = base_memory  # æ¢¯åº¦
    optim_memory = base_memory * 2  # AdamçŠ¶æ€
    activation_memory = 1.0  # æ¿€æ´»å€¼ä¼°ç®—

    total_peak = base_memory + grad_memory + optim_memory + activation_memory

    print(f"  â€¢ å³°å€¼æ˜¾å­˜: ~{total_peak:.1f} GB (ç†è®ºå€¼)")
    print(f"  â€¢ å®é™…å ç”¨: < {total_peak * 0.7:.1f} GB (ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹)")

def provide_optimization_suggestions():
    """æä¾›ä¼˜åŒ–å»ºè®®"""
    print("\nğŸ’¡ 22Gæ˜¾å­˜ä¼˜åŒ–å»ºè®®:")

    suggestions = [
        ("âœ… å·²å¯ç”¨", "æ¢¯åº¦æ£€æŸ¥ç‚¹ (gradient_checkpointing)"),
        ("âœ… å·²é…ç½®", "æ‰¹æ¬¡å¤§å°=1, æ¢¯åº¦ç´¯ç§¯=32"),
        ("âœ… å·²è®¾ç½®", "bfloat16ç²¾åº¦"),
        ("âœ… å·²å¯ç”¨", "Flash Attention"),
        ("âš ï¸  å¯è€ƒè™‘", "é™ä½LoRA rankåˆ°32 (èŠ‚çœæ˜¾å­˜)"),
        ("âš ï¸  å¯è€ƒè™‘", "å¢åŠ gradient_accumulation_stepsåˆ°64"),
        ("âš ï¸  å¯è€ƒè™‘", "ä½¿ç”¨DeepSpeed ZeRO-2"),
        ("âš ï¸  å¯è€ƒè™‘", "æ¨¡å‹é‡åŒ– (4-bit)")
    ]

    for status, suggestion in suggestions:
        print(f"  {status} {suggestion}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§  æ˜¾å­˜ç›‘æ§å’Œä¼˜åŒ–å»ºè®®")
    print("=" * 50)

    get_gpu_info()
    estimate_memory_usage()
    provide_optimization_suggestions()

    print("\nğŸš€ å½“å‰é…ç½®åº”è¯¥èƒ½åœ¨22Gæ˜¾å­˜ä¸Šè¿è¡Œ")
    print("å¦‚æœä»ç„¶æ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥è€ƒè™‘:")
    print("1. export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512")
    print("2. é™ä½LoRA rank: sed -i 's/64/32/g' config/train_config.json")
    print("3. ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡å¤§å°")

if __name__ == '__main__':
    main()
