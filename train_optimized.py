#!/usr/bin/env python3
"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±» - é«˜å‡†ç¡®ç‡ä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œä¼˜åŒ–çš„è¶…å‚æ•°
"""

import os
from typing import Dict, Any

# è®¾ç½®GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# ç›´æ¥å¯¼å…¥ - Swiftå®˜æ–¹æ¨èæ–¹å¼
from swift.llm import (
    TrainArguments, sft_main, register_dataset, DatasetMeta, ResponsePreprocessor, SubsetDataset
)

class FinancialSimilarityPreprocessor(ResponsePreprocessor):
    """é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦ä¸“ç”¨é¢„å¤„ç†å™¨"""

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        # ä¼˜åŒ–promptï¼Œå¢å¼ºé‡‘èé¢†åŸŸç†è§£
        query = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸‹é¢ä¸¤å¥è¯åœ¨é‡‘èè¯­å¢ƒä¸‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

å¥å­1: {row['text1']}
å¥å­2: {row['text2']}

è¯·åªè¾“å‡ºä¸€ä¸ªæ•°å­—ï¼š0æˆ–1
- 0: å«ä¹‰ä¸åŒæˆ–ä¸ç›¸ä¼¼
- 1: å«ä¹‰ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼

ä½ çš„å›ç­”ï¼š"""

        response = str(row['label'])

        row = {
            'query': query,
            'response': response
        }
        return super().preprocess(row)

# æ³¨å†Œæ•°æ®é›†
register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/financial_classification',
        subsets=[
            SubsetDataset('train', split=['train']),
            SubsetDataset('test', split=['test'])
        ],
        preprocess_func=FinancialSimilarityPreprocessor(),
    ))

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±»è®­ç»ƒ...")
    print("ğŸ“Š ä½¿ç”¨Qwen2-7Bæ¨¡å‹ï¼Œç›®æ ‡å‡†ç¡®ç‡: 0.87+")
    print("ğŸ’¾ æ˜¾å­˜è¦æ±‚: <22GB")

    # é«˜å‡†ç¡®ç‡ä¼˜åŒ–é…ç½®
    sft_main(TrainArguments(
        # ğŸ¯ å¤§æ¨¡å‹é€‰æ‹© - Qwen2-7Bæä¾›æ›´å¥½çš„æ€§èƒ½
        model='Qwen/Qwen2-7B-Instruct',

        # ğŸ“š æ•°æ®é›†é…ç½®
        dataset=['swift/financial_classification:train'],

        # ğŸ”§ è®­ç»ƒç±»å‹
        train_type='lora',

        # âš¡ ç²¾åº¦è®¾ç½®
        torch_dtype='bfloat16',

        # ğŸ“ˆ è®­ç»ƒè½®æ•° - å¢åŠ åˆ°5è½®æé«˜å‡†ç¡®ç‡
        num_train_epochs=5,

        # ğŸ“¦ æ‰¹æ¬¡å¤§å°ä¼˜åŒ– - åœ¨22Gæ˜¾å­˜ä¸‹æœ€å¤§åŒ–åˆ©ç”¨
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # 1*8=8æœ‰æ•ˆæ‰¹æ¬¡

        # ğŸ¯ å­¦ä¹ ç‡ä¼˜åŒ– - æ›´ç¨³å®šçš„æ”¶æ•›
        learning_rate=5e-5,  # ä»1e-4é™ä½åˆ°5e-5

        # ğŸ” LoRAé…ç½®ä¼˜åŒ– - æé«˜rankå¢å¼ºè¡¨è¾¾èƒ½åŠ›
        lora_rank=16,  # ä»8å¢åŠ åˆ°16
        lora_alpha=32,  # ä¿æŒalpha=2*rankæ¯”ä¾‹

        # ğŸ¯ ç›®æ ‡æ¨¡å— - å…¨é¢å¾®è°ƒ
        target_modules=['all-linear'],

        # ğŸ“ åºåˆ—é•¿åº¦ - é€‚åˆé‡‘èæ–‡æœ¬ç‰¹å¾
        max_length=512,  # ä»2048é™ä½åˆ°512ï¼Œé€‚åˆå¹³å‡13å­—ç¬¦æ–‡æœ¬

        # ğŸ’¾ å†…å­˜ä¼˜åŒ–
        attn_impl='flash_attn',
        packing=False,  # å…³é—­packingèŠ‚çœæ˜¾å­˜

        # ğŸ“Š è¯„ä¼°å’Œä¿å­˜
        eval_steps=100,
        save_steps=100,
        save_total_limit=3,

        # ğŸ“ æ—¥å¿—é¢‘ç‡
        logging_steps=10,

        # ğŸ”¥ é¢„çƒ­å’Œæ­£åˆ™åŒ–
        warmup_ratio=0.1,  # å¢åŠ é¢„çƒ­æ¯”ä¾‹

        # âš™ï¸ æ•°æ®å¤„ç†
        dataset_num_proc=4,
        dataloader_num_workers=2,  # å‡å°‘workeré¿å…å†…å­˜ç«äº‰

        # ğŸ“ è¾“å‡ºé…ç½®
        output_dir='output_qwen2_7b_optimized',
        save_only_model=True,

        # ğŸ¯ ç³»ç»Ÿæç¤º - å¢å¼ºé‡‘èé¢†åŸŸä¸“ä¸šæ€§
        system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ¤æ–­ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸¤å¥è¯åœ¨é‡‘èè¯­å¢ƒä¸‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œåªè¾“å‡º0æˆ–1ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚",
    ))

    print("âœ… è®­ç»ƒå®Œæˆï¼è¯·æ£€æŸ¥output_qwen2_7b_optimizedç›®å½•")
