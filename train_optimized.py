#!/usr/bin/env python3
"""
é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±» - é«˜å‡†ç¡®ç‡ä¼˜åŒ–ç‰ˆæœ¬
ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹å’Œä¼˜åŒ–çš„è¶…å‚æ•°
"""

# ===========================================
# ç´§æ€¥ä¿®å¤ï¼šdatasetså…¼å®¹æ€§é—®é¢˜
# åœ¨ä»»ä½•å¯¼å…¥ä¹‹å‰æ‰§è¡Œ
# ===========================================

print("ğŸ”§ [train] å¼€å§‹ç´§æ€¥ä¿®å¤datasetså…¼å®¹æ€§...")

try:
    # 1. ä¿®å¤pyarrowé—®é¢˜
    import pyarrow as pa
    print("ğŸ”§ [train] pyarrowä¿®å¤å®Œæˆ")

    if not hasattr(pa, 'PyExtensionType') and hasattr(pa, 'ExtensionType'):
        pa.PyExtensionType = pa.ExtensionType
        print("ğŸ”§ [train] å·²ä¿®å¤pyarrow.PyExtensionType")

    # 2. ä¿®å¤datasets LargeListé—®é¢˜
    import datasets
    print("ğŸ”§ [train] datasetsä¿®å¤å¼€å§‹")

    if not hasattr(datasets, 'LargeList'):
        try:
            from datasets.features import Sequence
            datasets.LargeList = Sequence
            print("ğŸ”§ [train] å·²ä¿®å¤datasets LargeList (ä½¿ç”¨Sequence)")
        except ImportError:
            class LargeList:
                def __init__(self, dtype, length=None):
                    self.dtype = dtype
                    self.length = length
            datasets.LargeList = LargeList
            print("ğŸ”§ [train] å·²åˆ›å»ºdatasets LargeListå…¼å®¹ç±»")

    # 3. ä¿®å¤_FEATURE_TYPES
    from datasets.features import features
    if not hasattr(features, '_FEATURE_TYPES'):
        _FEATURE_TYPES = {}
        for attr_name in dir(features):
            attr = getattr(features, attr_name)
            if (hasattr(attr, '__name__') and
                hasattr(attr, '__module__') and
                attr.__module__ == 'datasets.features.features' and
                (attr_name.endswith('Type') or 'Array' in attr_name or 'Value' in attr_name or 'Class' in attr_name)):
                _FEATURE_TYPES[attr_name] = attr

        if hasattr(features, 'Sequence'):
            _FEATURE_TYPES['LargeList'] = features.Sequence

        features._FEATURE_TYPES = _FEATURE_TYPES
        print(f"ğŸ”§ [train] å·²åˆ›å»º_FEATURE_TYPES ({len(_FEATURE_TYPES)}ä¸ªç±»å‹)")

    print("ğŸ”§ [train] æ‰€æœ‰å…¼å®¹æ€§ä¿®å¤å®Œæˆ")

except Exception as e:
    print(f"ğŸ”§ [train] ä¿®å¤å¤±è´¥: {e}")

print("ğŸ”§ [train] å¼€å§‹æ­£å¸¸å¯¼å…¥...\n")

# ===========================================
# æ­£å¸¸å¯¼å…¥å¼€å§‹
# ===========================================

import os
from typing import Dict, Any

# è®¾ç½®GPU
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

# ===========================================
# ä¿®å¤Swiftåº“å…¼å®¹æ€§é—®é¢˜
# ===========================================

print("ğŸ”§ ä¿®å¤Swiftåº“å…¼å®¹æ€§é—®é¢˜...")

try:
    # 1. ä¿®å¤transformers ALLOWED_LAYER_TYPES
    import sys
    try:
        import transformers
        print("ğŸ”§ ä¿®å¤transformerså…¼å®¹æ€§...")

        # ä¿®å¤ALLOWED_LAYER_TYPES
        if not hasattr(transformers.configuration_utils, 'ALLOWED_LAYER_TYPES'):
            transformers.configuration_utils.ALLOWED_LAYER_TYPES = [
                'Linear', 'Conv1D', 'Conv2d', 'Embedding', 'LayerNorm', 'Dropout'
            ]
            print("ğŸ”§ å·²æ·»åŠ  ALLOWED_LAYER_TYPES åˆ° transformers")

        # ä¿®å¤Gemma3Config
        if not hasattr(transformers, 'Gemma3Config'):
            # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é…ç½®ç±»
            from transformers.configuration_utils import PretrainedConfig
            class Gemma3Config(PretrainedConfig):
                model_type = "gemma3"
                def __init__(self, **kwargs):
                    super().__init__(**kwargs)
            transformers.Gemma3Config = Gemma3Config
            print("ğŸ”§ å·²æ·»åŠ  Gemma3Config åˆ° transformers")

    except ImportError:
        print("âš ï¸ transformersæœªå®‰è£…ï¼Œè·³è¿‡ä¿®å¤")

    # 2. ä¿®å¤lmdeploy EngineGenerationConfig
    try:
        import lmdeploy
        if not hasattr(lmdeploy, 'EngineGenerationConfig'):
            # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„å…¼å®¹ç±»
            class EngineGenerationConfig:
                def __init__(self, **kwargs):
                    for k, v in kwargs.items():
                        setattr(self, k, v)
            lmdeploy.EngineGenerationConfig = EngineGenerationConfig
            print("ğŸ”§ å·²æ·»åŠ  EngineGenerationConfig åˆ° lmdeploy")
    except ImportError:
        print("âš ï¸ lmdeployæœªå®‰è£…ï¼Œè·³è¿‡ä¿®å¤")

    print("âœ… Swiftåº“å…¼å®¹æ€§ä¿®å¤å®Œæˆ")

except Exception as e:
    print(f"âš ï¸ Swiftåº“ä¿®å¤å¤±è´¥ï¼Œç»§ç»­å°è¯•: {e}")

# ===========================================
# éªŒè¯ä¿®å¤æ•ˆæœ
# ===========================================

print("ğŸ” éªŒè¯ä¿®å¤æ•ˆæœ...")
try:
    # æµ‹è¯•datasetsæ˜¯å¦æ­£å¸¸
    import datasets
    assert hasattr(datasets, 'LargeList'), "LargeListä¸å­˜åœ¨"
    from datasets.features import features
    assert hasattr(features, '_FEATURE_TYPES'), "_FEATURE_TYPESä¸å­˜åœ¨"
    print("âœ… datasetså…¼å®¹æ€§ä¿®å¤éªŒè¯é€šè¿‡")
except Exception as e:
    print(f"âŒ datasetsä¿®å¤éªŒè¯å¤±è´¥: {e}")
    exit(1)

# ç°åœ¨å®‰å…¨åœ°å¯¼å…¥Swift
print("ğŸ”§ å¯¼å…¥Swift...")
try:
    from swift.llm import (
        TrainArguments, sft_main, register_dataset, DatasetMeta, ResponsePreprocessor, SubsetDataset
    )
    print("âœ… Swiftå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ Swiftå¯¼å…¥å¤±è´¥: {e}")
    # å¦‚æœSwiftå¯¼å…¥å¤±è´¥ï¼Œæä¾›æ›¿ä»£æ–¹æ¡ˆ
    print("ğŸ’¡ å°è¯•ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬...")
    try:
        # å°è¯•åªå¯¼å…¥éœ€è¦çš„éƒ¨åˆ†
        import swift
        print(f"âœ… SwiftåŸºç¡€å¯¼å…¥æˆåŠŸ (ç‰ˆæœ¬: {swift.__version__})")
        print("âš ï¸ ä½†llmæ¨¡å—å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜")
        print("ğŸ’¡ å»ºè®®:")
        print("1. æ£€æŸ¥Swiftç‰ˆæœ¬å…¼å®¹æ€§")
        print("2. æˆ–ä½¿ç”¨Swiftçš„å‘½ä»¤è¡Œå·¥å…·")
        print("3. æˆ–æ‰‹åŠ¨å®‰è£…å…¼å®¹ç‰ˆæœ¬çš„ä¾èµ–")
    except ImportError:
        print("âŒ Swiftå®Œå…¨ä¸å¯ç”¨")
    exit(1)

class FinancialSimilarityPreprocessor(ResponsePreprocessor):
    """é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦ä¸“ç”¨é¢„å¤„ç†å™¨"""

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        # ä¼˜åŒ–promptï¼Œå¢å¼ºé‡‘èé¢†åŸŸç†è§£
        query = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æœ¬åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸‹é¢ä¸¤å¥è¯åœ¨é‡‘èè¯­å¢ƒä¸‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚

å¥å­1: {row['text1']}
å¥å­2: {row['text2']}

è¯·åªè¾“å‡ºä¸€ä¸ªæ•°å­—ï¼š0æˆ–1
- 0: å«ä¹‰ä¸åŒæˆ–ä¸ç›¸ä¼¼
- 1: å«ä¹‰ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼

ä½ çš„å›ç­”ï¼š"""

        response = str(row['label'])

        row = {
            'query': query,
            'response': response
        }
        return super().preprocess(row)

# æ³¨å†Œæ•°æ®é›†
register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/financial_classification',
        subsets=[
            SubsetDataset('train', split=['train']),
            SubsetDataset('test', split=['test'])
        ],
        preprocess_func=FinancialSimilarityPreprocessor(),
    ))

if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ç±»è®­ç»ƒ...")
    print("ğŸ“Š ä½¿ç”¨Qwen2-7Bæ¨¡å‹ï¼Œç›®æ ‡å‡†ç¡®ç‡: 0.87+")
    print("ğŸ’¾ æ˜¾å­˜è¦æ±‚: <22GB")

    # é«˜å‡†ç¡®ç‡ä¼˜åŒ–é…ç½®
    sft_main(TrainArguments(
        # ğŸ¯ å¤§æ¨¡å‹é€‰æ‹© - Qwen2-7Bæä¾›æ›´å¥½çš„æ€§èƒ½
        model='Qwen/Qwen2-7B-Instruct',

        # ğŸ“š æ•°æ®é›†é…ç½®
        dataset=['swift/financial_classification:train'],

        # ğŸ”§ è®­ç»ƒç±»å‹
        train_type='lora',

        # âš¡ ç²¾åº¦è®¾ç½®
        torch_dtype='bfloat16',

        # ğŸ“ˆ è®­ç»ƒè½®æ•° - å¢åŠ åˆ°5è½®æé«˜å‡†ç¡®ç‡
        num_train_epochs=5,

        # ğŸ“¦ æ‰¹æ¬¡å¤§å°ä¼˜åŒ– - åœ¨22Gæ˜¾å­˜ä¸‹æœ€å¤§åŒ–åˆ©ç”¨
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # 1*8=8æœ‰æ•ˆæ‰¹æ¬¡

        # ğŸ¯ å­¦ä¹ ç‡ä¼˜åŒ– - æ›´ç¨³å®šçš„æ”¶æ•›
        learning_rate=5e-5,  # ä»1e-4é™ä½åˆ°5e-5

        # ğŸ” LoRAé…ç½®ä¼˜åŒ– - æé«˜rankå¢å¼ºè¡¨è¾¾èƒ½åŠ›
        lora_rank=16,  # ä»8å¢åŠ åˆ°16
        lora_alpha=32,  # ä¿æŒalpha=2*rankæ¯”ä¾‹

        # ğŸ¯ ç›®æ ‡æ¨¡å— - å…¨é¢å¾®è°ƒ
        target_modules=['all-linear'],

        # ğŸ“ åºåˆ—é•¿åº¦ - é€‚åˆé‡‘èæ–‡æœ¬ç‰¹å¾
        max_length=512,  # ä»2048é™ä½åˆ°512ï¼Œé€‚åˆå¹³å‡13å­—ç¬¦æ–‡æœ¬

        # ğŸ’¾ å†…å­˜ä¼˜åŒ–
        attn_impl='flash_attn',
        packing=False,  # å…³é—­packingèŠ‚çœæ˜¾å­˜

        # ğŸ“Š è¯„ä¼°å’Œä¿å­˜
        eval_steps=100,
        save_steps=100,
        save_total_limit=3,

        # ğŸ“ æ—¥å¿—é¢‘ç‡
        logging_steps=10,

        # ğŸ”¥ é¢„çƒ­å’Œæ­£åˆ™åŒ–
        warmup_ratio=0.1,  # å¢åŠ é¢„çƒ­æ¯”ä¾‹

        # âš™ï¸ æ•°æ®å¤„ç†
        dataset_num_proc=4,
        dataloader_num_workers=2,  # å‡å°‘workeré¿å…å†…å­˜ç«äº‰

        # ğŸ“ è¾“å‡ºé…ç½®
        output_dir='output_qwen2_7b_optimized',
        save_only_model=True,

        # ğŸ¯ ç³»ç»Ÿæç¤º - å¢å¼ºé‡‘èé¢†åŸŸä¸“ä¸šæ€§
        system="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æœ¬ç›¸ä¼¼åº¦åˆ¤æ–­ä¸“å®¶ã€‚è¯·ä»”ç»†åˆ†æä¸¤å¥è¯åœ¨é‡‘èè¯­å¢ƒä¸‹çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œåªè¾“å‡º0æˆ–1ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ã€‚",
    ))

    print("âœ… è®­ç»ƒå®Œæˆï¼è¯·æ£€æŸ¥output_qwen2_7b_optimizedç›®å½•")
